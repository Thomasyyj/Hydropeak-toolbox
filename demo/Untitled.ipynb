{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from datetime import datetime\n",
    "\n",
    "def readtemplate(path):\n",
    "    filename = path[-4:]\n",
    "    if filename == '.csv':\n",
    "        data = pd.read_csv(path, nrows=10000)\n",
    "    elif filename == 'xlsx':\n",
    "        data = pd.read_excel(path, nrows=10000)\n",
    "    return data\n",
    "\n",
    "\n",
    "def cal_sd(df):\n",
    "    drop_vec = []\n",
    "    coverage_time = 4  # refer to 4 time span of the time unit related to the data\n",
    "    downtemp = 0  # record whether decreasing happened\n",
    "    uptemp = 0  # record whether increasing happened\n",
    "    smooth_count = 0\n",
    "    for i in range(len(df['waterlevel']) - 1):\n",
    "        currvalue = df.at[i, 'waterlevel']\n",
    "        gradient = df.at[i + 1, 'waterlevel'] - currvalue\n",
    "        if gradient > 0:\n",
    "            if uptemp == 0:\n",
    "                startpoint = currvalue\n",
    "                uptemp = 1\n",
    "            elif smooth_count > coverage_time:\n",
    "                startpoint = currvalue\n",
    "                smooth_count = 0\n",
    "            else:\n",
    "                smooth_count = 0\n",
    "        elif gradient == 0 and uptemp == 1:\n",
    "            smooth_count += 1\n",
    "        elif gradient < 0 and uptemp == 1:\n",
    "            top = currvalue\n",
    "            drop_vec.append(top - startpoint)\n",
    "            uptemp = 0\n",
    "            smooth_count = 0\n",
    "    sd = np.sqrt(np.std(drop_vec))\n",
    "    return sd\n",
    "\n",
    "\n",
    "'''\n",
    "df: 数据集 dataset\n",
    "x1: 允许一次上升/下降的最长时长，若超过范围则用新的上升覆盖 (tolerance for the max time interval in one single incrasing/decreasing. If the time is beyond the tolerance, we replace the old one with the new one)\n",
    "x2: 用于筛选固定时间内的上升/下降值，若在时间范围内则保留，反之舍弃(tolerance for the max time interval between one hydropeak.)\n",
    "x3: 用于筛选固定落差，若小于该落差则被保留，大于该落差舍弃 (tolerance for the max drop between one hydropeak)\n",
    "x5: 检测的斜率阈值，若出现小于该值的斜率则不检测 (tolerance for the max gradient to be detected)\n",
    "'''\n",
    "\n",
    "\n",
    "def filterhydro(df, sd):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(df['time'], df['waterlevel'], 'k--', label='Before')\n",
    "    global time1, time2, bottom, top, uptemp, downtemp, gradient1, gradient2, timestart, timeend, valuestart, valueend, smooth_count\n",
    "    # Initialize the data\n",
    "    cov_time = 4  # refers to 4 interval of time related to the data\n",
    "    downtemp = 0  # record whether decreasing happened\n",
    "    uptemp = 0  # record whether increasing happened\n",
    "    start_val, top, end_val = 0, 0, 0\n",
    "    time1, time2 = 0, 0\n",
    "    smooth_count = 0\n",
    "    lock = 0\n",
    "    fit_count = 0\n",
    "    # filterfunction\n",
    "    for i in range(len(df['waterlevel'])-1):\n",
    "        currtime = i\n",
    "        currvalue = df.at[i, 'waterlevel']\n",
    "        # set the gradient\n",
    "        gradient = df.at[i+1, 'waterlevel'] - currvalue\n",
    "        # 遇到大幅度梯度上升，检测附近是否有成对出现的大幅度梯度下降，记录间隔与大小\n",
    "        # When an increasing gradient is detected, we want to find whether there is decreasing gradient nearby. We record the time interval and waterlevel difference\n",
    "        if uptemp != 1 or downtemp != 1:\n",
    "            if gradient > 0:\n",
    "                if uptemp == 0:\n",
    "                    time1 = currtime\n",
    "                    start_val = currvalue\n",
    "                    uptemp = 1\n",
    "                elif smooth_count > cov_time:\n",
    "                    time1 = currtime\n",
    "                    start_val = currvalue\n",
    "                    smooth_count = 0\n",
    "                else:\n",
    "                    smooth_count = 0\n",
    "            elif gradient == 0 and uptemp == 1:\n",
    "                smooth_count += 1\n",
    "            elif gradient < 0 and uptemp == 1:\n",
    "                top = currvalue\n",
    "                time2 = i + 1\n",
    "                end_val = df.at[i + 1, 'waterlevel']\n",
    "                downtemp = 1\n",
    "        # match\n",
    "        elif downtemp == 1 and uptemp == 1:\n",
    "            if gradient > 0:\n",
    "                leftdrop = top - start_val\n",
    "                rightdrop = top - end_val\n",
    "                if leftdrop <= sd and rightdrop <= sd:\n",
    "                    fillnum = (end_val - start_val) / (time2 - time1)\n",
    "                    for j in range(time1, time2 + 1):\n",
    "                        df.at[j, 'waterlevel'] = (\n",
    "                            j - time1) * fillnum + start_val\n",
    "                    uptemp = 0\n",
    "                    downtmp = 0\n",
    "                    lock = 0\n",
    "                    fit_count += 1\n",
    "                elif rightdrop >= sd and leftdrop < sd:\n",
    "                    for j in range(time1, time2 + 1):\n",
    "                        if df.at[j, 'waterlevel'] >= start_val:\n",
    "                            df.at[j, 'waterlevel'] = start_val\n",
    "                        else:\n",
    "                            break\n",
    "                    downtemp = 0\n",
    "                    uptemp = 0\n",
    "                    lock = 0\n",
    "                    fit_count += 1\n",
    "                elif leftdrop >= sd and rightdrop < sd:\n",
    "                    for j in range(time2, time1 - 1, -1):\n",
    "                        if df.at[j, 'waterlevel'] >= end_val:\n",
    "                            df.at[j, 'waterlevel'] = end_val\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    downtemp = 0\n",
    "                    fit_count += 1\n",
    "                    time1 = currtime\n",
    "                    start_val = currvalue\n",
    "                    uptemp = 1\n",
    "                lock = 0\n",
    "                smooth_count = 0\n",
    "                time1 = currtime\n",
    "                start_val = currvalue\n",
    "                uptemp = 1\n",
    "                downtemp = 0\n",
    "            elif gradient == 0:\n",
    "                smooth_count += 1  # record the times of no change\n",
    "            elif gradient < 0:\n",
    "                if smooth_count > cov_time:\n",
    "                    lock = 1  # lock the record of time and value if there are more than 4 times with no change\n",
    "                if lock != 1:\n",
    "                    time2 = currtime + 1\n",
    "                    end_val = df.at[i + 1, 'waterlevel']\n",
    "                    smooth_count = 0\n",
    "    print('We have filtered', fit_count, 'flowing within the sd')\n",
    "    ax.set_title('The waterlevel before and after filtering')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('water level')\n",
    "    ax.plot(df['time'], df['waterlevel'], label='After')\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "#    return df\n",
    "\n",
    "\n",
    "def filterhydro_plot(df, sd):\n",
    "    ctn = 0\n",
    "    while True:\n",
    "        filterhydro(df, sd)\n",
    "        ctn = input('Does the filter look plausible? If yes, input 1:\\n')\n",
    "        if ctn == '1':\n",
    "            break\n",
    "\n",
    "def datetime_preprocess(df):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df1 = df.set_index('time')\n",
    "    df1['num'] = 1\n",
    "    day = df1.to_period('D')\n",
    "    date_num = df1.resample('D').sum()\n",
    "    date_list = list(x.strftime('%d-%m-%Y') for x in date_num.index)\n",
    "    date_num = list(date_num['num'])\n",
    "    return date_num, date_list\n",
    "            \n",
    "def searchhydro(df, sd):\n",
    "    global time1, time2, bottom, top, uptemp, downtemp, timestart, timeend, valuestart, valueend, smooth_count\n",
    "    # Initialize the data\n",
    "    cov_time = 4  # refers to 4 interval of time related to the data\n",
    "    hydropeak_num = 0\n",
    "    downtemp = 0  # record whether decreasing happened\n",
    "    uptemp = 0  # record whether increasing happened\n",
    "    bottom, top = 0, 0\n",
    "    time1, time2 = 0, 0\n",
    "    timestart = []\n",
    "    timeend = []\n",
    "    valuestart = []\n",
    "    valueend = []\n",
    "    smooth_count = 0\n",
    "    rightdrop = []\n",
    "    leftdrop = []\n",
    "    rightslope = []\n",
    "    leftslope = []\n",
    "    peak_duration = []\n",
    "    peak_duration_count = 0\n",
    "    smooth_count = 0\n",
    "    \n",
    "    #import the time series\n",
    "    date_num, date_list = datetime_preprocess(df)\n",
    "    last_hydronum = 0\n",
    "    last_day = 0\n",
    "    day_count = 0 \n",
    "    hydronum_daily = {'number':[],'date':[]}\n",
    "    for i in range(len(df['waterlevel'])-1):\n",
    "        #append the hydronum\n",
    "        if i-last_day == date_num[day_count]-1:\n",
    "            hydronum_daily['number'].append(hydropeak_num-last_hydronum)\n",
    "            hydronum_daily['date'].append(date_list[day_count])\n",
    "            last_hydronum = hydropeak_num\n",
    "            last_day = i\n",
    "            if day_count < len(date_list)-1:\n",
    "                day_count += 1\n",
    "        elif i == len(df['waterlevel'])-1:\n",
    "            hydronum_daily['number'].append(hydropeak_num-last_hydronum)\n",
    "            hydronum_daily['date'].append(date_list[day_count])\n",
    "            last_hydronum = hydropeak_num\n",
    "        #start searching hydronum\n",
    "        currtime = i\n",
    "        currvalue = df.at[i, 'waterlevel']\n",
    "        # set the gradient\n",
    "        gradient = df.at[i+1, 'waterlevel'] - currvalue\n",
    "        # 遇到大幅度梯度上升，检测附近是否有成对出现的大幅度梯度下降，记录间隔与大小\n",
    "        # When an increasing gradient is detected, we want to find whether there is decreasing gradient nearby. We record the time interval and waterlevel difference\n",
    "        if uptemp != 1 or downtemp != 1:\n",
    "            if gradient > 0:\n",
    "                if uptemp == 0:\n",
    "                    time1 = currtime\n",
    "                    start_val = currvalue\n",
    "                    uptemp = 1\n",
    "                    peak_duration_count = 0\n",
    "                elif smooth_count > cov_time and uptemp == 1:\n",
    "                    time1 = currtime\n",
    "                    start_val = currvalue\n",
    "                    smooth_count = 0\n",
    "                    peak_duration_count = 0\n",
    "                else:\n",
    "                    smooth_count = 0\n",
    "                    peak_duration_count = 0\n",
    "            elif gradient == 0 and uptemp == 1:\n",
    "                smooth_count += 1\n",
    "                peak_duration_count += 1\n",
    "            elif gradient < 0 and uptemp == 1:\n",
    "                top = currvalue\n",
    "                toptime = i\n",
    "                time2 = currtime + 1\n",
    "                end_val = df.at[i + 1, 'waterlevel']\n",
    "                downtemp = 1\n",
    "                smooth_count = 0\n",
    "                peak_duration_count = 0\n",
    "            # match\n",
    "            \n",
    "        elif downtemp == 1 and uptemp == 1:\n",
    "            if gradient > 0:\n",
    "                hydropeak_num += 1\n",
    "                start_time = df.at[time1, 'time']\n",
    "                end_time = df.at[time2, 'time']\n",
    "                top_time = df.at[toptime, 'time']\n",
    "                timestart.append(start_time)\n",
    "                timeend.append(end_time)\n",
    "                valuestart.append(start_val)\n",
    "                valueend.append(end_val)\n",
    "                rttime = to_integer(top_time - start_time)\n",
    "                lttime = to_integer(end_time - top_time)\n",
    "                rightdrop.append(top - start_val)\n",
    "                leftdrop.append(top - end_val)\n",
    "                rightslope.append((top - end_val)/rttime)\n",
    "                leftslope.append((top - end_val)/lttime)\n",
    "                peak_duration.append(peak_duration_count)\n",
    "                downtemp = 0\n",
    "                time1 = currtime\n",
    "                start_val = currvalue\n",
    "                smooth_count = 0\n",
    "            elif gradient == 0:\n",
    "                smooth_count += 1\n",
    "            elif gradient < 0:\n",
    "                if smooth_count > cov_time:\n",
    "                    hydropeak_num += 1\n",
    "                    start_time = df.at[time1, 'time']\n",
    "                    end_time = df.at[time2, 'time']\n",
    "                    top_time = df.at[toptime, 'time']\n",
    "                    timestart.append(start_time)\n",
    "                    timeend.append(end_time)\n",
    "                    valuestart.append(start_val)\n",
    "                    valueend.append(end_val)\n",
    "                    rttime = to_integer(top_time - start_time)\n",
    "                    lttime = to_integer(end_time - top_time)\n",
    "                    rightdrop.append(top - start_val)\n",
    "                    leftdrop.append(top - end_val)\n",
    "                    rightslope.append((top - end_val)/rttime)\n",
    "                    leftslope.append((top - end_val)/lttime)\n",
    "                    peak_duration.append(peak_duration_count)\n",
    "                    uptemp = 0\n",
    "                    downtemp = 0\n",
    "                else:\n",
    "                    end_val = df.at[i + 1, 'waterlevel']\n",
    "                    time2 = currtime + 1\n",
    "                    smooth_count = 0\n",
    "    data_tocsv(leftdrop,rightdrop,timestart,timeend,rightslope,leftslope,peak_duration_count)    \n",
    "    daily_hydronum_tocsv(hydronum_daily)\n",
    "    print('The number of hydropeak is', hydropeak_num)\n",
    "    print('The sd is', sd)\n",
    "    plothydro(df)   \n",
    "\n",
    "def to_integer(datetime):\n",
    "    return int(datetime.total_seconds()/60)\n",
    "\n",
    "def daily_hydronum_tocsv(hydronum_daily):\n",
    "    hydronum_daily = pd.DataFrame(hydronum_daily)\n",
    "    hydronum_daily.to_csv('Daily_hydronum.csv')\n",
    "    \n",
    "def data_tocsv(leftdrop,rightdrop,timestart,timeend,rightslope,leftslope,peak_duration_count):\n",
    "    AllData = {\n",
    "        'leftdrop': leftdrop,\n",
    "        'rightdrop': rightdrop,\n",
    "        'starting_time':timestart,\n",
    "        'ending_time':timeend,\n",
    "        'rightslope':rightslope,\n",
    "        'leftslope':leftslope,\n",
    "        'peak_duration':peak_duration_count\n",
    "    }\n",
    "    AllData = pd.DataFrame(AllData,columns=['leftdrop',\n",
    "                                              'rightdrop',\n",
    "                                              'starting_time',\n",
    "                                             'ending_time',\n",
    "                                             'rightslope',\n",
    "                                             'leftslope',\n",
    "                                              'peak_duration'\n",
    "                                             ])\n",
    "    AllData.to_csv('Output.csv')\n",
    "\n",
    "def plothydro(df):\n",
    "    plt.title('list')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('water level')\n",
    "    plt.plot(df['time'], df['waterlevel'])\n",
    "    plt.scatter(timestart, valuestart, c='g')\n",
    "    plt.scatter(timeend, valueend, c='r')\n",
    "    plt.show()    \n",
    "\n",
    "def main():\n",
    "    register_matplotlib_converters()\n",
    "    print('start')\n",
    "    path = '../dataset/2152_Natural.xlsx'\n",
    "    # 确认输入文件类型符合且文件存在\n",
    "    while True:\n",
    "        filename = path[-4:]\n",
    "        if filename != '.csv' and filename != 'xlsx':\n",
    "            path = input('please input a excel or csv file\\n')\n",
    "        else:\n",
    "            try:\n",
    "                df = readtemplate(path)\n",
    "                break\n",
    "            except:\n",
    "                path = input(\n",
    "                    'Cannot find the file or there is something wrong. Please input again\\n')\n",
    "    try:\n",
    "        sd = 0.5 * cal_sd(df)\n",
    "        filterhydro_plot(df, sd)\n",
    "        searchhydro(df, sd)\n",
    "    except KeyError:\n",
    "        print('Please ensure there are columns named‘time’，‘waterlevel’\\n')\n",
    "    print('finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8e in position 16: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f5b82210ad7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dataset/2152_Natural.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8e in position 16: invalid start byte"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
